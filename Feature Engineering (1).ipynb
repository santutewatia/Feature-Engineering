{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94baa214",
   "metadata": {},
   "source": [
    "Q 1 What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2b35a",
   "metadata": {},
   "source": [
    "A parameter is a variable that is used to define or influence the behavior of a function, algorithm, model, or system. Parameters are typically inputs that are passed into a function or process, allowing it to perform specific computations or achieve specific results.\n",
    "\n",
    "Types of Parameters\n",
    "1. Function Parameters\n",
    "In programming, parameters are variables that a function accepts as input when it is called. These values are used to control or customize the function's behavior.\n",
    "\n",
    "2. Model Parameters\n",
    "In machine learning, parameters are variables that a model learns during the training process. These parameters determine how the model makes predictions.\n",
    "\n",
    "Examples of Model Parameters:\n",
    "\n",
    "Weights and biases in a neural network.\n",
    "Coefficients in linear regression.\n",
    "\n",
    "3. Hyperparameters\n",
    "These are parameters that are not learned from the data but are set before the training process begins to control the learning process.\n",
    "\n",
    "Examples of Hyperparameters:\n",
    "\n",
    "Learning rate.\n",
    "Number of layers or neurons in a neural network.\n",
    "Regularization strength.\n",
    "\n",
    "4. System Parameters\n",
    "These are variables that influence the behavior of a system, often passed to configure a system's operation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Configuration files in software development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c41b0d",
   "metadata": {},
   "source": [
    "Q 2 What is correlation?\n",
    "What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad6e34",
   "metadata": {},
   "source": [
    "\n",
    "Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It quantifies the strength and direction of a linear relationship between two variables.\n",
    "\n",
    "The relationship can be measured in terms of strength and direction:\n",
    "\n",
    "Strength: Indicates how closely the two variables are related (strong or weak relationship).\n",
    "Direction: Indicates whether the variables increase/decrease together (positive) or move in opposite directions (negative).\n",
    "\n",
    "Negative Correlation\n",
    "Negative correlation means that as one variable increases, the other decreases, and vice versa. This represents an inverse relationship between the two variables.\n",
    "\n",
    "Example:\n",
    "Variable A: Hours of exercise.\n",
    "Variable B: Weight.\n",
    "Typically, more hours of exercise (increase in Variable A) lead to a decrease in weight (Variable B).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c5def",
   "metadata": {},
   "source": [
    "Q 3 Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1566495",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and models that enable computers to learn patterns and make decisions or predictions based on data, without being explicitly programmed for specific tasks.\n",
    "Here's a breakdown of the key components in Machine Learning:\n",
    "\n",
    "1. Data\n",
    "Definition: The foundation of ML, consisting of structured or unstructured information used to train models.\n",
    "Types:\n",
    "Training Data: Used to train the model.\n",
    "Validation Data: Used to tune model parameters.\n",
    "Test Data: Used to evaluate model performance.\n",
    "\n",
    "2. Features\n",
    "Definition: Attributes or properties of the data that are used as input to the model.\n",
    "Feature Engineering: The process of selecting, transforming, or creating new features to improve model performance.\n",
    "\n",
    "3. Model\n",
    "Definition: A mathematical representation of a process or system that learns patterns from data.\n",
    "Examples: Linear regression, decision trees, neural networks.\n",
    "\n",
    "4. Algorithm\n",
    "Definition: A set of rules or procedures the model uses to learn from data.\n",
    "Types:\n",
    "Supervised Learning Algorithms: Learn from labeled data (e.g., Linear Regression, Decision Trees).\n",
    "Unsupervised Learning Algorithms: Learn patterns from unlabeled data (e.g., K-Means, PCA).\n",
    "Reinforcement Learning Algorithms: Learn through interaction with an environment (e.g., Q-Learning).\n",
    "\n",
    "5. Loss Function\n",
    "Definition: A metric that quantifies the difference between the predicted and actual values during training.\n",
    "Purpose: Guides the optimization process to minimize errors.\n",
    "\n",
    "6. Evaluation Metrics\n",
    "Definition: Metrics used to assess the model's performance on unseen data.\n",
    "Examples:\n",
    "Classification: Accuracy, Precision, Recall, F1 Score.\n",
    "Regression: Mean Squared Error (MSE), R-squared.\n",
    "7. Prediction:\n",
    "Once the model is trained and evaluated, it can be used to make predictions on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ce177",
   "metadata": {},
   "source": [
    "Q 4 How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c272a",
   "metadata": {},
   "source": [
    "In machine learning, the loss value is a crucial metric that helps determine how well a model is performing during training. It quantifies the difference between the model's predictions and the actual values in the data. In simpler terms, it tells us how \"wrong\" the model's predictions are.   \n",
    "\n",
    "Here's how the loss value helps in determining the quality of a model:\n",
    "\n",
    "1. Indicates the model's accuracy:\n",
    "\n",
    "A high loss value indicates that the model's predictions are far from the actual values, suggesting poor performance.   \n",
    "A low loss value indicates that the model's predictions are close to the actual values, suggesting good performance.\n",
    "\n",
    "2. Guides the training process:\n",
    "\n",
    "During training, the model's parameters are adjusted iteratively to minimize the loss value.   \n",
    "Optimization algorithms, such as gradient descent, use the loss value to determine the direction and magnitude of parameter adjustments.   \n",
    "By continuously minimizing the loss, the model learns to make more accurate predictions. \n",
    "\n",
    "3. Helps detect overfitting and underfitting:\n",
    "\n",
    "Overfitting occurs when the model learns the training data too well, including its noise and outliers. This results in a low loss on the training data but a high loss on unseen data.   \n",
    "Underfitting occurs when the model is too simple to capture the underlying patterns in the data, resulting in a high loss on both training and unseen data.   \n",
    "By monitoring the loss value on both training and validation data, we can detect overfitting and underfitting and take appropriate measures, such as adjusting the model's complexity or using regularization techniques.\n",
    "\n",
    "4. Enables comparison between models:\n",
    "\n",
    "Loss values can be used to compare the performance of different models on the same dataset.   \n",
    "The model with the lower loss value is generally considered to be better.\n",
    "\n",
    "In summary, the loss value is a fundamental metric in machine learning that provides valuable insights into a model's performance. By monitoring and minimizing the loss, we can train accurate models, detect potential problems like overfitting and underfitting, and compare the effectiveness of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c835bf2",
   "metadata": {},
   "source": [
    "Q 5 What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21f216",
   "metadata": {},
   "source": [
    "Variables in data analysis are typically classified into continuous and categorical based on the type of data they represent.\n",
    "\n",
    "1. Continuous Variables\n",
    "\n",
    "Represent quantities and are numeric in nature.\n",
    "Can take an infinite number of possible values within a range.\n",
    "Typically used to measure something, such as height, weight, or temperature.\n",
    "Examples:\n",
    "    \n",
    "Age: 25, 30.5, 40.8\n",
    "Salary: $45,000, $78,750\n",
    "Temperature: 23.4°C, 36.6°C\n",
    "    \n",
    "2. Categorical Variables\n",
    "\n",
    "Represent categories or groups.\n",
    "Can take a finite, distinct set of values or labels.\n",
    "Typically used to classify data into groups.\n",
    "\n",
    "Examples:\n",
    "Gender: Male, Female, Non-binary\n",
    "Education Level: High School, Bachelor’s, Master’s\n",
    "Region: North, South, East, West\n",
    "    \n",
    "Types of Categorical Variables:\n",
    "    \n",
    "Nominal: Categories without any inherent order.\n",
    "Example: Colors (Red, Blue, Green)\n",
    "    \n",
    "Ordinal: Categories with an inherent order.\n",
    "Example: Ratings (Poor, Average, Good, Excellent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8b54f",
   "metadata": {},
   "source": [
    "Q 6 How do we handle categorical variables in Machine Learning? What are the common t\n",
    "echniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b7afb",
   "metadata": {},
   "source": [
    "Handling categorical variables in Machine Learning is crucial because most machine learning algorithms can only work with numerical data. Categorical variables need to be converted into a numerical format to be processed by the models. Here are the common techniques used to handle categorical variables:\n",
    "\n",
    "1. Label Encoding\n",
    "Label encoding converts each unique category into an integer value. This method assigns a number to each category, making it possible for machine learning algorithms to interpret the data.\n",
    "\n",
    "2. One-Hot Encoding\n",
    "One-Hot Encoding creates binary (0 or 1) columns for each category in a categorical variable. Each category becomes a new column, with 1 indicating the presence of that category and 0 indicating its absence.\n",
    "\n",
    "3. Binary Encoding\n",
    "Binary encoding is a combination of label encoding and one-hot encoding. First, the categories are converted into integers, and then these integers are represented as binary numbers. Each binary digit becomes a new feature.\n",
    "\n",
    "4. Count or Frequency Encoding\n",
    "Count encoding replaces each category with the number of occurrences of that category in the dataset. Similarly, frequency encoding replaces categories with their frequency (relative occurrence).\n",
    "\n",
    "5. Target (Mean) Encoding\n",
    "Target encoding replaces each category with the mean of the target variable for that category. This method can help when there is a relationship between the categorical variable and the target.\n",
    "\n",
    "6. Embedding Layers (Deep Learning)\n",
    "In deep learning, embedding layers are used to convert categorical variables into dense vector representations. These embeddings are learned during training and capture semantic relationships between categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535fa4a",
   "metadata": {},
   "source": [
    "Q 7 What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130c462",
   "metadata": {},
   "source": [
    "Training and testing a dataset are crucial steps in building and evaluating machine learning models.\n",
    "\n",
    "1. Training a Dataset\n",
    "Training a dataset refers to the process of teaching a machine learning model by providing it with data, so it can learn patterns, relationships, and underlying structures from that data. The training dataset consists of input features (independent variables) and corresponding labels or outputs (dependent variable) that the model will use to learn.\n",
    "\n",
    "2. Testing a Dataset\n",
    "Testing a dataset refers to the process of evaluating the performance of a trained model using new, unseen data (i.e., test data). The test dataset is used to simulate real-world data that the model has not encountered before. The goal of testing is to assess how well the model generalizes to new, unseen data.\n",
    "\n",
    "Typical Data Split\n",
    "Training Data: 70-80% of the dataset\n",
    "Test Data: 20-30% of the dataset\n",
    "    \n",
    "Summary:\n",
    "Training: Teaching the model using labeled data.\n",
    "Testing: Evaluating the model on new, unseen data to assess its performance and generalization ability.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202bfca7",
   "metadata": {},
   "source": [
    "Q 8 What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b419212c",
   "metadata": {},
   "source": [
    "sklearn.preprocessing is a module in the scikit-learn library that provides a set of utilities and functions to preprocess data for machine learning models. Preprocessing refers to the steps taken to prepare raw data for analysis or modeling, ensuring that the data is in a format suitable for training machine learning algorithms. These preprocessing techniques can help improve the performance and effectiveness of a model.\n",
    "\n",
    "Summary:\n",
    "sklearn.preprocessing helps prepare raw data for machine learning by transforming it into a suitable form. This includes scaling features, encoding categorical variables, handling missing values, generating polynomial features, and more. Preprocessing is a vital step in the machine learning pipeline, as it can significantly impact the performance of models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a0bdc",
   "metadata": {},
   "source": [
    "Q 9 What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4821895",
   "metadata": {},
   "source": [
    "A Test set is a subset of data used to evaluate the performance of a trained machine learning model. After the model has been trained on the training set, it is tested on the test set to measure how well it generalizes to new, unseen data.\n",
    "\n",
    "Key Points:\n",
    "    \n",
    "Purpose: To validate the model's performance on data it hasn't seen before.\n",
    "    \n",
    "Size: Typically, the test set comprises 20-30% of the original dataset.\n",
    "    \n",
    "Evaluation: Metrics like accuracy, precision, recall, or mean squared error are calculated on the test set.\n",
    "    \n",
    "Importance: A good performance on the test set indicates that the model is not overfitting the training data and can generalize to real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad628a",
   "metadata": {},
   "source": [
    "Q 10 How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcc913",
   "metadata": {},
   "source": [
    "\n",
    "How do we split data for model fitting (training and testing) in Python?\n",
    "Splitting data into training and testing sets is a critical step to evaluate a machine learning model's performance. In Python, this can be achieved using the train_test_split function from the sklearn.model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dd9557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 120\n",
      "Test set size: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944d0bf",
   "metadata": {},
   "source": [
    "How do you approach a Machine Learning problem?\n",
    "\n",
    "Define the problem.\n",
    "Collect and preprocess data.\n",
    "Perform EDA.\n",
    "Train and evaluate models.\n",
    "Optimize and deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fceb830",
   "metadata": {},
   "source": [
    "Q 11 Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a51886",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is an essential step in the machine learning workflow that helps us understand the dataset better before building any models. It ensures data quality and reveals critical insights that impact the model's performance.\n",
    "\n",
    "\n",
    "EDA (Exploratory Data Analysis) helps:\n",
    "\n",
    "Identify patterns and relationships in data.\n",
    "\n",
    "Detect anomalies and outliers\n",
    "\n",
    "Guide preprocessing and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ddaf9",
   "metadata": {},
   "source": [
    "Q 12 What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e31758",
   "metadata": {},
   "source": [
    "Correlation measures the relationship between two variables. It indicates the extent to which one variable changes when the other changes.\n",
    "\n",
    "Positive correlation: Both variables increase or decrease together.\n",
    "    \n",
    "Negative correlation: One variable increases as the other decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abecbf",
   "metadata": {},
   "source": [
    "Q 13 What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edd4f9",
   "metadata": {},
   "source": [
    "Negative Correlation Negative correlation means that as one variable increases, the other decreases, and vice versa. This represents an inverse relationship between the two variables.\n",
    "\n",
    "Example: Variable A: Hours of exercise. Variable B: Weight. Typically, more hours of exercise (increase in Variable A) lead to a decrease in weight (Variable B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36eed0",
   "metadata": {},
   "source": [
    "Q 14 How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183324a",
   "metadata": {},
   "source": [
    "To find the correlation between variables in Python, you can use libraries like Pandas and NumPy, which provide built-in methods for calculating correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61beff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Variable1  Variable2  Variable3\n",
      "Variable1        1.0        1.0       -1.0\n",
      "Variable2        1.0        1.0       -1.0\n",
      "Variable3       -1.0       -1.0        1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'Variable1': [10, 20, 30, 40],\n",
    "    'Variable2': [15, 25, 35, 45],\n",
    "    'Variable3': [40, 30, 20, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0333edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = [10, 20, 30, 40]\n",
    "y = [15, 25, 35, 45]\n",
    "\n",
    "# Compute correlation\n",
    "correlation = np.corrcoef(x, y)\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bd2b4",
   "metadata": {},
   "source": [
    "Q 15 What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3239df",
   "metadata": {},
   "source": [
    "Causation means that one event directly causes another event.\n",
    "\n",
    "Correlation means that two events occur together but don't necessarily have a cause-effect relationship.\n",
    "\n",
    "Example: There may be a correlation between ice cream sales and drowning accidents during summer, but this doesn't mean ice cream sales cause drowning. Both are influenced by warmer weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b5ed1",
   "metadata": {},
   "source": [
    "Q 16 What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b202cd2",
   "metadata": {},
   "source": [
    "An optimizer is an algorithm used to minimize the loss function by adjusting the model’s parameters. Common types include:\n",
    "\n",
    "Gradient Descent: Iteratively adjusts parameters by calculating the gradient of the loss function.\n",
    "    \n",
    "Stochastic Gradient Descent (SGD): Updates parameters based on a single data point, making it faster but more noisy.\n",
    "    \n",
    "Adam (Adaptive Moment Estimation): A combination of gradient descent with momentum and adaptive learning rates, widely used in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa7308",
   "metadata": {},
   "source": [
    "Q 17 What is sklearn.linear_model ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c43cc5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "sklearn.linear_model is a module in the scikit-learn library that contains a collection of algorithms for solving linear regression and classification problems. These models assume a linear relationship between the input variables (features) and the target variable(s).\n",
    "\n",
    "Common Models in sklearn.linear_model:\n",
    "\n",
    "LinearRegression: For predicting continuous values.\n",
    "\n",
    "LogisticRegression: For binary or multi-class classification.\n",
    "\n",
    "Ridge: Linear regression with L2 regularization to reduce overfitting.\n",
    "\n",
    "Lasso: Linear regression with L1 regularization for feature selection.\n",
    "\n",
    "ElasticNet: Combines L1 and L2 regularization\n",
    "\n",
    "SGDClassifier and SGDRegressor: Use stochastic gradient descent for classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e27d849",
   "metadata": {},
   "source": [
    "Q 18 What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118da45",
   "metadata": {},
   "source": [
    "The model.fit() method is used to train a machine learning model on a given dataset. It takes the input data (features) and the corresponding target values (labels), and then it adjusts the model parameters (weights) to minimize the error or loss.\n",
    "\n",
    "In supervised learning, fit() is used to train the model by finding the best parameters that allow the model to make accurate predictions.\n",
    "\n",
    "Arguments for model.fit()\n",
    "\n",
    "X (features): This is the input data or the independent variables used to train the model. It can be a 2D array, pandas DataFrame, or matrix where rows represent samples and columns represent features.\n",
    "\n",
    "y (target/labels): This is the target data or dependent variable, representing the outcomes associated with each sample in the input data. For classification, it contains the class labels, and for regression, it contains continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c66a3e",
   "metadata": {},
   "source": [
    "Q 19 What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f12d09",
   "metadata": {},
   "source": [
    "The model.predict() method in machine learning is used to make predictions based on the trained model. Once the model has been trained using model.fit(), model.predict() is used to apply the model to new data (i.e., unseen test data) to make predictions. The output will be the predicted values or labels based on the learned patterns from the training dataset.\n",
    "\n",
    "Arguments:\n",
    "    \n",
    "X (features): The input data for which the predictions are to be made. This argument should be in the same format as the data used for training (typically a 2D array or DataFrame). Each row represents a sample, and each column represents a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc515f",
   "metadata": {},
   "source": [
    "Q 20 What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1709f",
   "metadata": {},
   "source": [
    "Variables in data analysis are typically classified into continuous and categorical based on the type of data they represent.\n",
    "\n",
    "1 Continuous Variables\n",
    "Represent quantities and are numeric in nature. Can take an infinite number of possible values within a range. Typically used to measure something, such as height, weight, or temperature. Examples:\n",
    "\n",
    "Age: 25, 30.5, 40.8 Salary:  45,000,\n",
    " 78,750 Temperature: 23.4°C, 36.6°C\n",
    "\n",
    "2 Categorical Variables\n",
    "Represent categories or groups. Can take a finite, distinct set of values or labels. Typically used to classify data into groups.\n",
    "\n",
    "Examples: Gender: Male, Female, Non-binary Education Level: High School, Bachelor’s, Master’s Region: North, South, East, West\n",
    "\n",
    "Types of Categorical Variables:\n",
    "\n",
    "Nominal: Categories without any inherent order. Example: Colors (Red, Blue, Green)\n",
    "\n",
    "Ordinal: Categories with an inherent order. Example: Ratings (Poor, Average, Good, Excellent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736459e",
   "metadata": {},
   "source": [
    "Q 21 What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31f585",
   "metadata": {},
   "source": [
    "Feature scaling is the process of standardizing or normalizing the range of independent variables or features in a dataset. In many machine learning algorithms, the scale of the data can affect the performance and accuracy of the model. Scaling the features ensures that each feature contributes equally to the learning process, and models that rely on distance (e.g., k-Nearest Neighbors, Support Vector Machines, etc.) perform better when the features are scaled.\n",
    "\n",
    "How Does Feature Scaling Help in Machine Learning?\n",
    "Improves Model Performance\n",
    "\n",
    "Faster Convergence in Gradient Descent\n",
    "\n",
    "Equal Weight for All Features\n",
    "\n",
    "Improved Accuracy for Distance-Based Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14463be",
   "metadata": {},
   "source": [
    "Q 22 How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70625a55",
   "metadata": {},
   "source": [
    "We can scale features using StandardScaler or MinMaxScaler from sklearn.preprocessing.\n",
    "\n",
    "Conclusion:\n",
    "    \n",
    "StandardScaler: Used when features have different units or variances.\n",
    "    \n",
    "MinMaxScaler: Used when you need to scale data to a specific range, typically [0, 1\n",
    "                                                                               \n",
    "MaxAbsScaler: Scales each feature by its maximum absolute value.\n",
    "                                                                               \n",
    "RobustScaler: Scales using the median and IQR, useful in the presence of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f1d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daaf681",
   "metadata": {},
   "source": [
    "Q 23 What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1852e43",
   "metadata": {},
   "source": [
    "sklearn.preprocessing is a module in the scikit-learn library that provides a set of utilities and functions to preprocess data for machine learning models. Preprocessing refers to the steps taken to prepare raw data for analysis or modeling, ensuring that the data is in a format suitable for training machine learning algorithms. These preprocessing techniques can help improve the performance and effectiveness of a model.\n",
    "\n",
    "Summary: sklearn.preprocessing helps prepare raw data for machine learning by transforming it into a suitable form. This includes scaling features, encoding categorical variables, handling missing values, generating polynomial features, and more. Preprocessing is a vital step in the machine learning pipeline, as it can significantly impact the performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8396d",
   "metadata": {},
   "source": [
    "Q 24 How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a245b44",
   "metadata": {},
   "source": [
    "In Python, the most common way to split data for model fitting (training and testing) is by using the train_test_split function from the sklearn.model_selection module. This function splits your dataset into two sets: a training set and a test set. The training set is used to train the model, and the test set is used to evaluate its performance on unseen data.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df4833",
   "metadata": {},
   "source": [
    "Q 25 Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb075af",
   "metadata": {},
   "source": [
    "Data encoding refers to converting categorical data into a numerical format that can be used by machine learning algorithms. Common methods include:\n",
    "\n",
    "Label encoding: Assigning a unique integer to each category.\n",
    "    \n",
    "One-hot encoding: Creating binary columns for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245179a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08dc313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
